# Practical Recipies for Your Data Processing

Subtitle: The 80/20 Guide that Solves Your Data Cleaning, Exploratory Data Analysis and Feature Engineering with Tabular Datasets

Thanks to the powerful packages we have available nowadays, training machine learning models is often a very tiny step in the pipeline of a regular data science project. Altogether, we need to address the following tasks:

1. Data Understanding & Formulation of the Questions
2. Data Cleaning
3. Exploratory Data Analysis
4. Feature Engineering
5. Feature Selection
6. Data Modelling

Additionally, if life inferences are planned, several parts from the steps 2-5 needs to be prepared for production environments, i.e., they need to be transferred into scripts in which reproducibility and maintainability can be guaranteed for robust and trustworthy deployments.

Independently from that fact and remaining still in the research and development environment, steps 2-5 consume a large percentage of the effort. We need to apply some kind of methodical creativity to often messy datasets that almost never behave as we initially want.

So, is there an easy way out? Unfortunately, I'd say there is not. However, I have collected a series of guidelines you can use systematically to ease your journey. This blog post summarizes the map I have drawn along the years.

My dedicated [Github repository](https://github.com/mxagar/eda_fe_summary) contains a large python script `data_processing.py` which collects many **code snippets** that achieve 80% of the processing work I usually do with **tabular** datasets. As a companion, 


## Data Cleaning



## Exploratory Data Analysis



## Feature Engineering



## Feature Selection



## Data Modelling



## Tips for Production


